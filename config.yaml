# Lucy Voice Configuration

# Whisper ASR
whisper_model_name: "Systran/faster-whisper-small"
whisper_device: "cpu"
whisper_compute_type: "int8"
whisper_language: "es"
whisper_task: "transcribe"
whisper_force_language: true

# Ollama LLM
ollama_model: "gpt-oss:20b"
# Modelo fusionado opcional (LoRA multilingüe)
ollama_model_fused: "gpt-oss-20b-multireasoner"
ollama_host: "http://localhost:11434"

# TTS
tts_voice: "es_ES/m-ailabs_low#karen_savage"

# Audio / VAD
sample_rate: 16000
channels: 1
dtype: "float32"
record_seconds_fixed: 4.0
vad_aggressiveness: 2
vad_frame_duration_ms: 30
vad_min_speech_ms: 300
vad_silence_duration_ms: 900
vad_min_utterance_ms: 1800
silence_duration_stop: 1.0
max_record_seconds: 10.0

# Wake Word (ahora con soporte mejorado)
wake_word:
  enabled: true  # Set to false to disable wake word and use continuous listening
  confidence_threshold: 0.5  # Adjust 0.0-1.0; lower = more sensitive
  cooldown_seconds: 2.0  # Time after detection before listening for next wake word
  # Model paths: empty list uses default OpenWakeWord models (hey_jarvis, alexa)
  # To use custom model: add path like ["models/hola_lucy.onnx"]
  model_paths: []  # Use defaults or specify custom models
  
# Búsqueda web
web_search:
  provider: "searxng"
  searxng_url: "http://127.0.0.1:8080"
  language: "es-AR"
  safesearch: 1
  top_k: 5
  fetch_top_n: 3
  timeout_s: 12

# --- Voz modular de Lucy (v2) ---

voice_modular:
  enabled: true
  whisper_model: "base"
  vad_sample_rate: 16000
  vad_aggressiveness: 2
  sleep_commands:
    - "lucy dormi"
    - "lucy dormí"
  # Half-duplex mode: mute microphone during TTS playback
  half_duplex: true

# LLM Model Management
# Current model (already configured above as ollama_model)
# To switch models, change ollama_model value and restart Lucy
# Example alternatives (ensure model is installed via 'ollama pull'):
#   - "llama2:7b"      (~4 GB VRAM, faster but less capable)
#   - "llama2:13b"     (~8 GB VRAM, balanced)
#   - "gpt-oss:20b"    (~13 GB VRAM, current default)
#   - "llama2:70b-q4"  (~35 GB VRAM, most capable but needs quantization)
# 
# For LoRA fine-tuning:
#   1. Train LoRA adapter with your dataset
#   2. Save as new model: ollama create my-custom-model -f Modelfile
#   3. Set ollama_model: "my-custom-model"
# See docs/LUCY-LLM-GPT-OSS-LORA.md for details
