Resumen Técnico del Proyecto Lucy – Asistente de Voz Local
Resumen Ejecutivo

Lucy es un asistente de voz local diseñado para funcionar completamente offline en una PC potente, capaz de escuchar comandos hablados en español, entender la intención mediante un modelo de lenguaje, ejecutar acciones en el sistema (si corresponde) y responder al usuario con voz sintetizada. El flujo completo abarca desde la detección de la frase de activación “hola Lucy” hasta la respuesta por voz: Lucy oye al usuario, transcribe lo que dice, interpreta la petición con un modelo de IA, actúa mediante herramientas de automatización (cuando es necesario) y finalmente responde hablando. El objetivo del proyecto es lograr un asistente de escritorio robusto para uso diario, que responda de forma natural en castellano rioplatense y pueda realizar tareas en la máquina bajo órdenes vocales. Lucy se basa en una arquitectura modular donde cada componente (desde el micrófono hasta el sintetizador de voz) cumple un rol específico, coordinados por un pipeline de orquestación. En resumen, Lucy combina un modelo lingüístico avanzado (“mente”) con herramientas de sistema (“manos”) para brindar una experiencia de asistente de voz integral y controlable enteramente de forma local.

Componentes y Módulos Principales

A continuación se listan los principales componentes del sistema Lucy y sus funciones:

Micrófono & Captura de Audio: Lucy utiliza el micrófono de la PC para escuchar continuamente el entorno en busca de la palabra de activación. La librería sounddevice se emplea para capturar audio en tiempo real desde el micrófono. (En el futuro se integrará también un detector de actividad de voz VAD para determinar automáticamente cuándo el usuario termina de hablar).

Wake Word (“hola Lucy”): Un módulo de OpenWakeWord (OWW) está entrenado para detectar la frase clave “hola Lucy” que activa al asistente. Hasta que se escuche esta frase, Lucy permanece en estado de baja actividad escuchando solo el wake word. Al detectar “hola Lucy”, se inicia el proceso de grabación y procesamiento del comando de voz del usuario. Este enfoque evita procesar audio continuamente y reduce falsos activadores.

ASR (Automatic Speech Recognition): Para convertir la voz del usuario en texto, Lucy integra Whisper a través de la librería faster-whisper (una implementación optimizada). Se utiliza un modelo pre-entrenado Whisper Small en español, corriendo en CPU en modo cuantizado int8 para maximizar rendimiento. El ASR toma el audio posterior a la palabra de activación y produce la transcripción textual de la petición del usuario, con precisión suficiente para comandos en castellano.

LLM (Modelo de Lenguaje – “mente”): El núcleo cognitivo de Lucy es un modelo de lenguaje grande ejecutándose localmente. Actualmente se emplea el modelo gpt-oss:20b (20 mil millones de parámetros) alojado en el servicio Ollama. Este LLM interpreta la transcripción del usuario, determina la respuesta apropiada o la acción a realizar, y genera una respuesta en texto. El LLM está configurado mediante un prompt de sistema para comportarse como un asistente útil en español rioplatense, dando respuestas concisas y conversacionales (por ejemplo, respuestas orales breves, sin formato Markdown, máximo dos frases salvo que se le pida más detalle). Lucy se ejecuta completamente offline, aprovechando el hardware local para correr el modelo; de hecho, el modelo de 13 GB se mantiene cargado en la GPU para inferencia rápida. En fases futuras se contempla alternar entre este modelo “pesado” y modelos más pequeños (e.g. 7–8B parámetros como Mistral o Llama 2) para reducir latencia en tiempo real.

LucyTools (Herramientas de Acción – “manos”): Son un conjunto de funciones Python que permiten a Lucy manipular el sistema y realizar acciones de escritorio bajo instrucción del LLM. Residen en el módulo lucy_voice/lucy_tools.py. Entre las primeras herramientas implementadas están: abrir_aplicacion(...) para lanzar aplicaciones, simular_teclado(...) para enviar pulsaciones de teclas, mover_raton(...) para mover el cursor, y tomar_captura(...) para hacer capturas de pantalla. Estas funciones usan la librería pyautogui para interactuar con el sistema operativo. Cada herramienta está programada con comprobaciones básicas de error y seguridad (por ejemplo, validar si una aplicación existe, manejar coordenadas de pantalla fuera de rango, etc.) para evitar comportamientos impredecibles.

Orquestador / Pipeline de Voz: El flujo completo de Lucy está gestionado por una clase principal LucyVoicePipeline (definida en lucy_voice/pipeline_lucy_voice.py) que coordina los módulos anteriores. Inicialmente, esta clase implementa las funciones en forma secuencial (grabación de audio, transcripción, llamada al LLM, síntesis de respuesta, etc.) y provee métodos de prueba como run_text_roundtrip(user_text) para consultas de texto a texto, run_mic_to_text_once() para probar solo ASR, y run_mic_llm_roundtrip_once() para ejecutar el ciclo completo de voz a voz. En una versión posterior, se integrará el framework Pipecat (un sistema de pipeline para agentes conversacionales) para implementar estos pasos como nodos en un grafo de procesamiento en tiempo real. Con Pipecat, cada componente (micrófono, wake word, ASR, LLM, TTS, etc.) será un nodo concurrente, lo que facilitará manejar estados (p. ej. pausar la escucha durante la respuesta hablada) y futuras extensiones como VAD avanzado o incluso entrada de video. Actualmente, Pipecat ya está incluido en las dependencias y se ha verificado su importación, pero el pipeline completo está en transición hacia ese modelo.

TTS (Text-to-Speech): Para convertir la respuesta del LLM a audio, Lucy emplea el motor TTS Mimic 3 de Mycroft. Se ha configurado una voz en español (es_ES/carlfm_low) para que la respuesta suene natural en castellano. El módulo de síntesis se invoca desde el pipeline (método _speak_with_tts) que llama al binario mimic3 con la voz seleccionada, y luego reproduce el archivo de audio resultante mediante aplay en los parlantes del sistema. Esto le da a Lucy la capacidad de “hablar” con una voz computarizada entendible. En futuras iteraciones se planea mejorar la calidad de la voz utilizando el sistema Coqui TTS (modelo XTTS-v2) aprovechando la GPU, una vez que el pipeline sea estable.

Interfaz de Usuario (GUI) [En desarrollo]: Si bien la interacción primaria es por voz, se prevé agregar una interfaz visual mínima (posiblemente con Gradio u otra librería) para depuración y control. Esta GUI mostraría el historial de conversación (transcripciones del usuario y respuestas de Lucy) y el estado interno del sistema (p. ej. indicadores de “escuchando”, “pensando”, “hablando” o “ejecutando herramienta”). Aunque no es prioritaria en estética, esta capa ayuda a monitorear el funcionamiento en tiempo real y a confirmar qué entiende Lucy y qué acciones realiza. Por ahora, también se interactúa con Lucy mediante una consola (modo chat de texto para pruebas) y se cuentan con scripts de shell para lanzar pruebas de voz rápida (por ejemplo, scripts/lucy_voice_mic_roundtrip.sh ejecuta una ronda completa de hablar y escuchar).

Flujo de Datos Paso a Paso

A continuación se describe el flujo típico de datos e interacciones en Lucy, desde que el usuario inicia un comando de voz hasta que recibe una respuesta hablada. Este proceso cubre tanto el pipeline de audio como la posible ejecución de herramientas:

Escucha Pasiva (Wake Word): Lucy permanece a la escucha continua del audio ambiente esperando oír la frase “hola Lucy”. Un detector de wake word (OpenWakeWord) procesa el audio en tiempo real con baja latencia hasta reconocer esa secuencia específica. Mientras no se detecta, el resto del pipeline está inactivo para ahorrar recursos.

Activación y Grabación: Al detectar “hola Lucy”, el sistema se activa. Inmediatamente comienza a grabar el audio del micrófono durante la frase/comando que el usuario pronuncia a continuación. En esta fase, Lucy captura la voz del usuario completo (por ejemplo, “¿qué hora es?”) en un búfer de audio. Se puede emplear un mecanismo de VAD (Voice Activity Detection) para determinar el final de la locución del usuario automáticamente, o alternativamente grabar por una duración máxima predefinida (ej.: 5 segundos en las pruebas iniciales) y luego detener.

Transcripción de Voz a Texto (ASR): Una vez que el usuario termina de hablar, el segmento de audio capturado se envía al módulo ASR (Whisper). Aquí, la librería faster-whisper procesa el audio y genera la transcripción en texto de lo que dijo el usuario. Este texto suele ser una o varias frases en español que representan la pregunta o comando del usuario (por ejemplo: “Lucy, ¿qué hora es?” -> “qué hora es”). La transcripción puede incluir también un indicador de confianza o idioma detectado; el sistema lo registra para fines de logging y depuración. El resultado final de esta etapa es una cadena de texto limpia (user_text) que será pasada al modelo de lenguaje.

Entrada al Modelo de Lenguaje: El texto transcrito se entrega al LLM (vía Ollama) junto con el prompt de sistema predefinido. Lucy’s pipeline invoca el modelo mediante la interfaz de Ollama (por ejemplo usando un comando CLI ollama run gpt-oss:20b) y envía un mensaje de usuario con el contenido a interpretar. El prompt de sistema recuerda al LLM que es un asistente de voz y puede incluir una lista de herramientas disponibles en formato JSON (esquema de tool calling) para que el LLM sepa cómo pedir acciones externas. El LLM entonces procesa la entrada y genera una respuesta en lenguaje natural o una llamada a herramienta, dependiendo de la petición:

Si la consulta del usuario es informativa o conversacional, el LLM normalmente producirá directamente una respuesta en texto (por ejemplo, “Son las 3 de la tarde”).

Si la consulta requiere interactuar con el sistema (por ejemplo “abre el navegador” o “toma una captura de pantalla”), el LLM, siguiendo su entrenamiento/instrucciones de herramienta, puede generar como salida una estructura especial (ej. un JSON) indicando la intención de usar una herramienta específica de LucyTools en lugar de una respuesta textual final.

Detección de Llamada a Herramienta: El orquestador (LucyVoicePipeline/Pipecat) analiza la salida del LLM. Si la respuesta del modelo contiene una instrucción de herramienta (por ejemplo un objeto JSON con el nombre de la función y parámetros), entonces Lucy interrumpe el flujo de respuesta directa y procede a ejecutar esa acción. En cambio, si el LLM devolvió texto normal, el flujo salta directamente al paso de síntesis de voz (paso 7).

Ejecución de la Acción (LucyTools): Cuando el LLM pide una herramienta, el pipeline llama a la función correspondiente en el módulo LucyTools para realizar la acción en el sistema operativo. Por ejemplo, si el modelo devolvió { "tool": "abrir_aplicacion", "app": "calculadora" }, Lucy ejecutará internamente abrir_aplicacion("calculadora"). Estas funciones efectúan acciones reales (abrir una aplicación, tomar una captura de pantalla, etc.) utilizando pyautogui u otras APIs del sistema. Cada herramienta retorna un resultado o confirmación. Por ejemplo, tomar_captura() podría devolver el path del archivo de imagen generado, o abrir_aplicacion podría retornar un mensaje de éxito o error. El resultado de la herramienta se captura para integrarlo en la respuesta. (Cabe destacar que durante esta ejecución, el LLM está “pensando” o esperando; Lucy no escuchará nuevos comandos hasta completar la acción actual).

Nueva Interpretación (Post-Acción): Tras completar la acción, el pipeline alimenta al LLM con la información del resultado. Es decir, se realiza un segundo paso de inferencia donde se brinda al modelo el contexto de que la herramienta fue ejecutada y qué ocurrió (por ejemplo: “Se ha tomado la captura de pantalla y guardado en ~/Imagenes/captura1.png”). Entonces el LLM genera ahora la respuesta final al usuario, explicando el resultado de la acción en lenguaje natural (“Listo, tomé una captura de pantalla y la guardé en tu carpeta de Imágenes”). De esta forma, la mente (LLM) toma conciencia del efecto de sus manos (herramientas) y puede comunicarlo apropiadamente. Nota: Si no se trataba de una acción sino una respuesta directa, este paso simplemente consiste en usar la primera salida textual del LLM como respuesta final.

Síntesis de Voz (Texto a Voz): Una vez determinado el texto final que Lucy debe comunicar al usuario, entra en juego el módulo TTS. El pipeline pasa la respuesta (p.ej. “Son las tres de la tarde”) al motor Mimic 3 para sintetizarla en audio. Mimic 3 genera un archivo de sonido WAV con la locución en español (voz femenina/masculina según la voz elegida). Acto seguido, el sistema reproduce ese audio en los altavoces mediante una herramienta de reproducción (aplay en Linux). Durante esta etapa, Lucy está en modo “hablando”: se detiene temporalmente cualquier nueva escucha para que no se active a sí misma con su propia voz.

Salida de Audio: El usuario escucha la respuesta hablada de Lucy que debe sonar natural y relevante a su pregunta. Por ejemplo, ante “¿qué hora es?”, Lucy (vía TTS) podría responder: “Son las tres de la tarde.” con entonación neutra en español rioplatense. Si se ejecutó una herramienta, también informará el resultado por voz: por ejemplo “Aquí tienes la captura de pantalla en tu carpeta de Imágenes.”.

Retorno a Escucha (Half-Duplex): Al terminar de hablar, Lucy vuelve al estado de escucha pasiva con el wake word activado. Gracias a un manejo half-duplex, el sistema asegura no escuchar ni transcribir durante su propia locución (para evitar bucles de retroalimentación). Ahora, con la respuesta completada, el ciclo puede reiniciar: Lucy espera nuevamente “hola Lucy” para atender la siguiente consulta. Internamente, se manejan flags de estado o indicadores para saber si está en modo “listening” (escuchando), “thinking” (procesando/LLM) o “speaking” (hablando), de modo que los distintos componentes del pipeline se habilitan o pausan según corresponda en cada momento.

(Los pasos anteriores describen el flujo simplificado una vez que el sistema esté totalmente integrado. Durante el desarrollo, existen scripts de prueba para verificar partes individuales: e.g., grabar 5s de audio y transcribir, o enviar texto al LLM y ver la respuesta en consola. Sin embargo, el objetivo final es que todo ocurra automáticamente como se describió.)

Interacción entre “Mente” (LLM) y “Manos” (Acciones)

Un principio clave en el diseño de Lucy es la separación de responsabilidades entre el modelo de lenguaje y las herramientas de acción, concepto llamado de forma ilustrativa “mente y manos”. La mente (el LLM) es la encargada de comprender la intención del usuario y decidir qué hacer o responder, mientras que las manos (las funciones de LucyTools y otros comandos del sistema) son las que efectivamente ejecutan la acción en la computadora. Esta arquitectura estilo agent garantiza control y seguridad: el LLM no actúa directamente sobre el sistema operativo, sino que sus “deseos” deben pasar por un filtro de herramientas explícitamente permitidas.

En la práctica, la interacción mente-manos ocurre mediante un protocolo de Tool Calling simple: al LLM se le provee un listado de las herramientas disponibles y el formato para llamarlas (por ejemplo, un JSON con campos específicos). Cuando el modelo determina que para cumplir la petición del usuario necesita usar una herramienta, en lugar de dar una respuesta final, genera ese JSON indicando qué función invocar y con qué parámetros. Por ejemplo, ante la orden “Lucy, abre la calculadora”, el LLM podría devolver algo como: { "tool": "abrir_aplicacion", "app": "Calculadora" }. El orquestador detecta este formato y delega a LucyTools la ejecución de abrir_aplicacion("Calculadora"). Una vez ejecutado, el resultado (éxito al abrir la app o error si no existe) se pasa de vuelta al LLM para que este continúe la conversación. En la respuesta final, el modelo integrará ese resultado, diciendo algo al usuario como “He abierto la Calculadora”.

Durante todo este ciclo, el LLM permanece como el director intelectual: decide si hace falta una acción externa y cuál, mientras que LucyTools actúa como los efectores físicos en la PC. Si una herramienta produce un resultado complejo (p. ej. texto extraído de una captura de pantalla en el futuro, o información de una API local), ese contenido puede también ser inyectado al prompt para que el LLM lo procese y lo explique en lenguaje natural al usuario. Importante es que cada función de LucyTools está diseñada para devolver un resumen claro de lo ocurrido (o del error en caso de fallo), de forma que el LLM pueda simplemente comunicar ese resumen por voz. Por ejemplo, tomar_captura() tras guardar el archivo podría retornar una frase “Captura realizada en ~/Imagenes/cap.png”, que el modelo incluirá (posiblemente reformulada) en su respuesta hablada.

Esta separación mente-manos facilita también la extensibilidad: se pueden agregar nuevas herramientas (nuevas funciones en LucyTools) y describirlas en el prompt del LLM para dotar a Lucy de más capacidades, sin necesidad de re-entrenar el modelo. Mientras la descripción y el formato esperado se proporcionen al LLM, este podrá decidir usarlas correctamente. Asimismo, se controla mejor la seguridad porque el LLM no puede realizar acciones fuera de las previstas – por ejemplo, no puede ejecutar código arbitrario a menos que exista una herramienta explícita para ello en LucyTools y se le haya dado acceso. En resumen, la “mente” de Lucy tiene conocimiento y capacidad de decisión, pero solo puede hacer cosas a través de las “manos” que el desarrollador le ha dado, manteniendo al sistema bajo control deliberado.

Detalles de Entorno y Versiones

Lucy está desarrollada y desplegada en un entorno controlado, con hardware y software orientados a aprovechar modelos de IA locales de alto rendimiento. A continuación se detallan los aspectos relevantes del entorno:

Hardware: Máquina local de alta gama con CPU AMD Ryzen 9 7950X de 16 núcleos (32 hilos) y 128 GB de RAM, equipada con GPU NVIDIA GeForce RTX 5090 con ~32 GB de VRAM. Esta capacidad de cómputo permite ejecutar modelos grandes de IA (20B parámetros) íntegramente en la GPU sin depender de servicios en la nube. De hecho, el modelo principal de Lucy reside cargado en la VRAM (~13 GB en uso) para inferencia rápida.

Sistema Operativo: Ubuntu 24.04.3 LTS (Linux 6.14.x) de 64 bits, configurado como desktop estándar. El sistema tiene instalados y activos los servicios necesarios, incluyendo Ollama (servidor local de modelos de lenguaje) y Docker (para posibles contenedores), entre otros. Ubuntu provee un entorno estable y compatibilidad con las librerías de audio y GPU utilizadas.

Entorno de Desarrollo (Python): Lucy se implementa principalmente en Python 3.12 dentro de un entorno virtual dedicado (.venv-lucy-voz) dentro del repositorio. Las dependencias de Python se gestionan via pip (v24) instalando los requisitos listados en lucy_voice/requirements.txt. Entre las librerías clave instaladas están:

faster-whisper (implementación optimizada de Whisper para ASR) – usando modelo small en CPU int8.

mycroft-mimic3-tts (sintetizador TTS local) – con voz española seleccionada.

openwakeword (detección de wake word con modelo personalizado para “hola Lucy”).

pipecat-ai (framework de orquestación de agentes de voz) – preparado para el pipeline.

sounddevice (captura y playback de audio) – usado para micro y posiblemente para salida en lugar de aplay si se integra directamente.

pyautogui (automatización GUI) – permite a LucyTools controlar mouse/teclado, etc..

Otras libs auxiliares: numpy, wave (procesamiento de audio), etc. (todas las anteriores confirmadas funcionando en pruebas individuales).

Servicio LLM Local: Se utiliza Ollama v0.12.9 como backend local para el modelo de lenguaje. Ollama corre como servicio en el puerto local 11434, al cual Lucy (o la extensión VS Code) envía solicitudes. El modelo principal cargado es gpt-oss:20b, de ~13 GB, que está afinado para seguir instrucciones en español. Este modelo fue elegido por su equilibrio entre capacidad y licencia abierta. Adicionalmente, en la fase de planificación se contempló instalar modelos más pequeños como Llama 3.1 8B instruct o Mistral 7B para escenarios de baja latencia, manteniendo gpt-oss:20b como referencia de mayor calidad. El modelo opera completamente en la GPU local (NVIDIA RTX 5090), aprovechando la VRAM abundante. No se requiere conexión a internet para ninguna inferencia; todo el procesamiento es en local, lo que brinda privacidad y reduce la latencia de ida/vuelta.

Otros Detalles: El entorno de desarrollo está gestionado con VS Code, y el proyecto Lucy (incluyendo el módulo de voz lucy_voice/) está versionado en Git. Existe una extensión de asistencia de código (“Continue”) configurada para usar el modelo local via Ollama, de modo que incluso el desarrollo del proyecto se apoya en la propia infraestructura de IA local. Para pruebas de audio, se utilizan utilidades del sistema como arecord/aplay en Linux, y se han escrito scripts Bash para lanzar el asistente o pruebas rápidas. Toda la configuración (comandos de instalación, dependencias, snapshots del sistema) está documentada en la carpeta docs/ del repositorio para poder reinstalar o actualizar el entorno fácilmente.

Consideraciones Especiales y Decisiones de Diseño

El desarrollo de Lucy ha implicado diversas decisiones de diseño pensadas para lograr un asistente de voz eficiente, seguro y agradable de usar en el día a día:

Operación 100% Local: Un principio fundamental es que Lucy funcione sin depender de servicios externos. Esto garantiza privacidad (las conversaciones no salen de la máquina) y disponibilidad sin costos recurrentes. A cambio, se requiere hardware potente (lo cual se tiene) y optimizaciones para que los modelos locales respondan con baja latencia. Esta autonomía también inspiró la filosofía “mente-manos”, para mantener bajo control qué puede hacer el modelo.

Arquitectura Modular y Fases de Integración: El proyecto se dividió en fases claras de desarrollo. En Fase 1 se instalaron y probaron por separado todas las piezas (ASR, TTS, wake word, LLM, herramientas), asegurando que cada componente funcionara aislado. En Fase 2 se enfocó en integrar estos componentes en un pipeline unificado (con Pipecat) y lograr el ciclo completo voz→voz con algunas acciones básicas. La Fase 3 planeada se centra en refinar y robustecer: optimizar tiempos, mejorar precisión y agregar funcionalidades avanzadas. Esta estrategia por etapas permitió detectar problemas temprano en cada módulo antes de combinarlos, facilitando la depuración. Además, el código está organizado en módulos (lucy_voice, lucy_tools, tests, docs) y bajo control de versiones, con buena documentación de cada hito, lo que sienta una base clara para futuras expansiones.

Rendimiento y Latencia: Dado que la interacción es en tiempo real (conversación), se han tomado decisiones para reducir la latencia en cada paso. Por ejemplo, usar el modelo Whisper small en modo int8 para ASR, sacrifica algo de precisión pero gana velocidad. El LLM de 20B, si bien es pesado, se mantiene cargado en memoria para evitar demoras de carga, y se evalúa cuantizarlo o reemplazarlo por uno más pequeño si la demora de inferencia resulta muy alta. Del lado de TTS, Mimic3 es relativamente rápido, pero se considera implementar cachés de audio para frases repetidas o precomputar ciertas respuestas comunes. También se mide cuidadosamente el tiempo entre que el usuario termina de hablar y Lucy empieza a responder, ajustando parámetros (tamaño de chunk de Whisper, etc.) para minimizarlo. La meta de diseño es que Lucy responda lo más instantáneamente posible, idealmente en pocos segundos, para una experiencia conversacional fluida.

Half-Duplex y Manejo de Estados: Un desafío propio de los asistentes de voz es evitar que el sistema se escuche a sí mismo y cause un bucle. Lucy implementa un esquema half-duplex estricto: nunca escucha mientras está hablando. Esto se logró introduciendo estados internos (listening, thinking, speaking) y banderas que pausan la captura de micrófono durante la síntesis/reproducción de audio. En diseño se decidió controlar esto a nivel del pipeline (y en el futuro con Pipecat será aún más preciso), para asegurar que el wake word y el ASR se reactiven solo cuando la respuesta de Lucy haya terminado por completo. De esta forma se evitan activaciones falsas por eco y se mejora la UX (Lucy no interrumpe ni se pisa con el usuario).

Precisión vs. Falsos Positivos (Wake Word): Otro aspecto de diseño es la calibración del detector de “hola Lucy”. Se optó por un modelo personalizado con OpenWakeWord para la frase de activación, y se están ajustando los umbrales de detección para equilibrio entre sensibilidad y especificidad. Un wake word poco sensible haría que Lucy “no escuche” al usuario, y uno muy sensible generaría activaciones no deseadas con palabras parecidas. Por ello, se planea recopilar más ejemplos (incluso sintéticos) de la frase y de ruidos de fondo para refinar el modelo y posiblemente reentrenarlo si se observan falsos positivos/negativos frecuentes. Este énfasis muestra la intención de hacer a Lucy confiable en entornos reales (ej. que no se active con la radio o conversaciones ajenas).

Idioma y Personalidad: Lucy está pensada para el castellano rioplatense (variante de español de Argentina/Uruguay). Esto se refleja en la voz TTS elegida y en el prompt del LLM que le indica responder en ese dialecto. Así, las respuestas de Lucy usan expresiones y entonación naturales para el usuario objetivo. Además, la personalidad de Lucy en el prompt es la de una asistente local amable y directa. Se decidió limitar las respuestas por defecto a un par de frases, evitando divagaciones, ya que en interacción por voz las respuestas concisas suelen ser más efectivas. Solo si el usuario lo solicita, Lucy daría explicaciones más largas. Esta directriz (no divagar, no formatear en Markdown, etc.) fue importante para adaptar un modelo de lenguaje genérico a un uso conversacional por voz.

Robustez y Manejo de Errores: En una herramienta que interactúa con el sistema, es crucial manejar errores de forma segura y comprensible. Se decidió programar cada función de LucyTools con validaciones: p.ej., abrir_aplicacion comprueba si la aplicación existe o está duplicada antes de ejecutar; tomar_captura verifica permisos y rutas; todas capturan excepciones imprevistas. Si algo falla (por ejemplo, “no pude encontrar la aplicación X”), la función retorna un mensaje de error que el LLM comunicará al usuario en términos claros. Así Lucy podrá decir “Lo siento, no pude abrir X porque no está instalada” en lugar de simplemente no hacer nada. Esta filosofía de failing gracefully fue una decisión de diseño para incrementar la confianza del usuario en el asistente. Asimismo, se contemplan pruebas intensivas de las herramientas (intentar abrir apps inexistentes, hacer clic fuera de la pantalla, etc.) para asegurar que esos casos estén cubiertos. Los resultados de estas pruebas guiarán ajustes adicionales antes de considerarla “lista para uso diario”.

Documentación y Mantenibilidad: Finalmente, un aspecto notable es la extensa documentación y control de versiones desde el inicio. Se mantienen archivos Markdown con snapshots de configuración del sistema, instrucciones de instalación reproducibles, y un registro diario de avances en el documento del proyecto. Esto no solo ayuda a cualquier colaborador (o al assistant de programación como Codex) a entender el estado actual, sino que también facilita volver a configurar todo en caso de reinstalación de la máquina. Decisiones clave (como elección de modelos, parámetros de audio, etc.) están justificadas en la documentación. Para operar Lucy cómodamente, se planea incluir scripts de arranque y parada (start_lucy.sh, stop_lucy.sh) que automaticen la activación de todos los servicios necesarios. Todas estas decisiones buscan que el proyecto Lucy sea sostenible en el tiempo: fácil de actualizar, de depurar y de mejorar por cualquier desarrollador futuro con contexto técnico.

En conclusión, Lucy representa una integración cuidadosa de tecnologías de voz, lenguaje e interacción con el sistema, con un claro reparto de responsabilidades entre la “mente” inteligente (LLM) y las “manos” que efectúan acciones. Cada componente ha sido elegido y configurado con miras a un asistente de voz en español que funcione en tiempo real, de forma local, y con posibilidades de expansión. Este resumen sirve como base de conocimiento para orientar futuras instrucciones de desarrollo: conociendo esta arquitectura, un asistente programador (como Codex) puede comprender dónde y cómo incorporar nuevas herramientas, mejorar el pipeline de voz o refactorizar loops lógicos con máxima precisión y alineación al diseño original del proyecto.